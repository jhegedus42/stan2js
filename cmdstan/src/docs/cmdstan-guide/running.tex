\chapter{Running a CmdStan Program}\label{stan-cmd.chapter}

\noindent
Once a CmdStan program is compiled, it can be run in many different
ways.  It can be used to sample or optimize parameters, or to diagnose
a model.  Before diving into the detailed configurations, the first
section provides some simple examples.


\section{Getting Started by Example}\label{command-getting-started.section}

Once a CmdStan program has been converted to a \Cpp program for that
model (see \refchapter{stanc}) and the resulting \Cpp program compiled
to a platform-specific executable (see \refchapter{compiling}),
the model is ready to be run.

All of the CmdStan functionality is highly configurable from the command
line; the options are defined later in this chapter.  Each command
option also has defaults, which are used in this section.

\subsection{Sampling}

Suppose the executable is in file \code{my\_model} and the data is in
file \code{my\_data}, both in the current working directory.  To
generate samples from a data set using the default settings, use one
of the following, depending on platform.

\subsubsection{Mac OS and Linux}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./my_model sample data file=my_data
\end{Verbatim}
\end{quote}

\subsubsection{Windows}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> my_model sample data file=my_data
\end{Verbatim}
\end{quote}
%
On both platforms, this command reads the data from file
\code{my\_data}, runs warmup tuning for 1000 iterations (the values of
which are discarded), and then runs the fully-adaptive \NUTS sampler
for 1000 iterations, writing the parameter (and other) values to the
file \code{samples.csv} in the current working directory.  When no
random number seed is specified, a seed is generated from the system
time.

\subsection{Sampling in Parallel}

The previous example executes one chain, which can be repeated to
generate multiple chains. However, users may want to execute chains
in parallel on a multicore machine.

\subsubsection{Mac OS and Linux}

To sample four chains using a Bash shell on Mac OS or Linux, execute%
%
\footnote{Complicated multiline commands such as this one are prime candidates for putting into a script file.}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> for i in {1..4}
  do
    ./my_model sample random seed=12345 \
       id=$i data file=my_data \
       output file=samples$i.csv &
 done
\end{Verbatim}
\end{quote}
%
The ampersand (\code{\&}) at the end of the nested command pushes each process into the background, so that the loop can continue without waiting for the current chain to finish.  The \code{id} value makes sure that a non-overlapping set of random numbers are used for each chain.  Also note that the output file is explicitly specified, with the variable \code{\$i} being used to ensure the output file name for each chain is unique.

The terminal standard output will be interleaved for all chains
running concurrently.  To suppress all terminal output, direct the
standard output to the ``null'' device.  This is achieved by
postfixing \code{> /dev/null} to a command, which in the above case,
means changing the second-to-last line to
\begin{quote}
\begin{Verbatim}
       output file=samples$i.csv > /dev/null &
\end{Verbatim}
\end{quote}



\subsubsection{Windows}

On Windows, the following is functionally equivalent to the Bash
snippet above
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> for /l %x in (1, 1, 4) do start /b model  sample   ^
   random seed=12345 id=%x data file=my_data         ^
   output file=samples%x.csv
\end{Verbatim}
\end{quote}
%
The caret (\code{\textasciicircum}) indicates a line continuation in
DOS.

\subsubsection{Combining Parallel Chains}

CmdStan has commands to analyze the output of multiple chains, each
stored in their own file; see \refchapter{stansummary}.  RStan also
has commands to read in multiple CSV files produced by CmdStan's
command-line sampler.

To compute posterior quantities, it is sometimes easier to have the
chains merged into a single CSV file.  If the grep and sed programs
are installed, then the following will combine the four
comma-separated values files into a single comma-separated values
file.  The command is the same on Windows, Mac OS, and Linux.
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> grep lp__ samples1.csv > combined.csv
> sed '/^[#l]/d' samples*.csv >> combined.csv
\end{Verbatim}
\end{quote}

\subsubsection{Scripting and Batching}

The previous examples show how to sample in parallel from the command
line.  Operations like these can also be scripted, using shell scripts
(\code{.sh}) on Mac OS and Linux and DOS batch (\code{.bat}) files on
Windows.  A sequence of several such commands can be executed from a
single script file.  Such scripts might contain \code{stanc} commands
(see \refchapter{stanc}) and \code{stansummary} commands (see
\refchapter{stansummary}) can be executed from a single script file.
At some point, it is worthwhile to move to something with stronger
dependency control such as makefiles.


\subsection{Optimization}

CmdStan can find the posterior mode (assuming there is one).  If the
posterior is not convex, there is no guarantee Stan will be able to
find the global mode as opposed to a local optimum of log probability.

For optimization, the mode is calculated without the Jacobian
adjustment for constrained variables, which shifts the mode due to the
change of variables.  Thus modes correspond to modes of the model as
written.

\subsubsection{Windows}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> my_model optimize data file=my_data
\end{Verbatim}
\end{quote}

\subsubsection{Mac OS and Linux}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./my_model optimize data file=my_data
\end{Verbatim}
\end{quote}


\subsection{Variational Inference}

CmdStan can fit a variational approximation to the posterior. The approximation
is a Gaussian in the unconstrained variable space. Stan implements two
variational algorithms. The \code{algorithm=meanfield} option uses a fully
factorized Gaussian for the approximation. The \code{algorithm=fullrank} option
uses a Gaussian with a full-rank covariance matrix for the approximation.

\subsubsection{Mac OS and Linux}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./my_model variational algorithm=meanfield  \
             data file=my_data
> ./my_model variational algorithm=fullrank   \
             data file=my_data
\end{Verbatim}
\end{quote}

\subsubsection{Windows}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> my_model variational algorithm=meanfield    \
           data file=my_data
> my_model variational algorithm=fullrank     \
           data file=my_data
\end{Verbatim}
\end{quote}

\section{Diagnostics}\label{diagnostics.section}

CmdStan has a basic diagnostic feature that will calculate gradients of
the initial state and compare them with those calculated with finite
differences.  If there are discrepancies, there is a problem with the
model or initial states (or a bug in Stan).  To run on the different
platforms, use one of the following.

\subsubsection{Mac OS and Linux}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./my_model diagnose data file=my_data
\end{Verbatim}
\end{quote}

\subsubsection{Windows}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> my_model diagnose data file=my_data
\end{Verbatim}
\end{quote}


\section{Generate Quantities}

CmdStan can be used to generate additional quantities of interest
given a model, data, and a sample drawn from the model conditioned
on the data.
For each draw in the sample, CmdStan runs the \code{generated quantities}
block using the parameter estimates for that draw as the parameter values.

\subsubsection{Mac OS and Linux}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./my_model generate_quantities fitted_params=my_samples.csv \
           data file=my_data
\end{Verbatim}
\end{quote}

\subsubsection{Windows}
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> my_model generate_quantities fitted_params=my_samples.csv \
           data file=my_data
\end{Verbatim}
\end{quote}


\section{Command-Line Options}\label{stan-command-line-options.section}

A CmdStan program can be run in many different ways, e.g., sampling,
optimization, variational inference.  As these different uses require
different configuration options, CmdStan has its own syntax for
specifying the configuration options in a way that follows the logical
dependencies between the Stan program and the way in which is it to be
run.  The command-line options are specified as a set of top-level set of
general configuration categories which take further sets
of keyword-value pairs as sub-arguments.  The top-level categorical
keywords are:
%
\begin{itemize}
\itemsep0em
\item{\code{method} - required, see below}
\item{\code{id}  - optional, value is non-negative integer, specified as \code{id=\farg{int-value}}}
\item{\code{random} - optional, single keyword with sub-arguments}
\item{\code{data} - optional, , single keyword with sub-arguments}
\item{\code{init} - optional, , single keyword with sub-arguments}
\item{\code{output} - optional, , single keyword with sub-arguments}
\end{itemize}
%
CmdStan also provides a \code{help} option described in the next section.

The \code{method} argument has a nested set of sub-categories
where all sub-arguments are appropriate to the choice of the method argument. 
Default values are provided for all sub-argument options.
All sub-arguments for a given argument can be specified in any order.
Because \code{method} is the only mandatory argument, it can be
specified either as \code{method=\farg{method\_name}} or simply as
\farg{method\_name}, as seen in previous examples, i.e., the keywords
\code{sample}, \code{optimize}, \code{variational}, \code{diagnose},
or \code{generate\_quantities}.


\subsection{Help}

Informative output can be retrieved either globally, by requesting help
at the top-level, or locally, by requesting help deeper into the hierarchy.
Note that after any help has been displayed the execution immediately
terminates, even if a method has been specified.

If \code{help} is specified as the only argument then a usage message is
displayed.  Similarly, specifying \code{help\_all} by itself displays the entire
argument hierarchy.
Specifying \code{help} after any argument displays a description and
valid options for that argument.  For example,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
./my_model method=sample help
\end{Verbatim}
\end{quote}
%
provides the top-level options for the \code{sample} method.

Detailed information on the argument, and all arguments deriving
from it, can accessed by specifying \code{help-all} instead,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
./my_model method=sample help-all
\end{Verbatim}
\end{quote}

\subsection{Method}

All commands other than \code{help} must include at least one
method, specified explicitly as \code{method=\farg{method\_name}} or
implicitly with only \farg{method\_name}.  Currently CmdStan supports
the following methods:
%
\begin{center}
\begin{tabular}{r|l}
{\it Method} & {\it Description} \\ \hline \hline
{\tt sample}   &  sample using MCMC
\\
{\tt optimize} &  find posterior mode using optimization
\\
{\tt variational} &  fit variational approximation (experimental)
\\
{\tt diagnose} &  diagnose models
\\
{\tt generate\_quantities}  &generate quantities of interest
\end{tabular}
\end{center}
%

\section{Full Argument Hierarchy}\label{detailed-command-arguments.section}

Here we present the full argument hierarchy, along with relevant details.
Some typical use-case examples are provided in the next section.

\subsection{Typographical Conventions}

The following typographical conventions are obeyed in the hierarchy.
%
\begin{itemize}
\item \code{arg=\farg{<value-type>}}
\\
Arguments with values; displays the value type, legal values, and default value
%
\item \code{\bfseries arg}
\\
Isolated categorical arguments; displays all valid subarguments
%
\item \code{\farg{value}}
\\
Values; describes effect of selecting the value
%
\item \code{\farg{\bfseries avalue}}
\\
Categorical arguments that appear as values to other arguments; displays all valid subarguments
\end{itemize}

\subsection{Top-Level Method Argument}

Every command must have exactly one method specified.
The value type of \code{list element} means that the valid
values are enumerated as a list.

\begin{description}
\hiercmdarg{}{method}{$$list element$$}
  {Analysis method (Note that \code{method=} is optional)}
  {Valid values: \  \code{sample, optimize, variational, diagnose, generate\_quantities}}
  {Defaults to \code{sample}}
\end{description}

\subsection{Sampling-Specific Arguments}

The following arguments are specific to sampling.  The method argument
\code{sample} (or \code{method=sample}) must come first in order to
enable the subsequent arguments.  The other arguments are optional and
may appear in any order.

\begin{description}
  \hierlongcmd{\indentarrow}{\farg{\bfseries sample}}
    {Bayesian inference with Markov Chain Monte Carlo}
    {Valid subarguments:
      \code{num\_samples, num\_warmup, save\_warmup,
        \\ \hspace*{24pt} thin, adapt, algorithm}}
%
    \hiercmdarg{\indentarrow\indentarrow}{num\_samples}{$$int$$}
      {Number of sampling iterations}
      {Valid values: \  $0 \leq \mbox{\code{num\_samples}}$}
      {Defaults to \code{1000}}
%
    \hiercmdarg{\indentarrow\indentarrow}{num\_warmup}{$$int$$}
      {Number of warmup iterations}
      {Valid values: \  $0 \leq \mbox{\code{warmup}}$}
      {Defaults to \code{1000}}
%
    \hiercmdarg{\indentarrow\indentarrow}{save\_warmup}{$$boolean$$}
      {Stream warmup samples to output?}
      {Valid values: \ \code{0, 1}}
      {Defaults to \code{0}}
%
    \hiercmdarg{\indentarrow\indentarrow}{thin}{$$int$$}
      {Period between saved samples}
      {Valid values: \  $0 < \mbox{\code{thin}}$}
      {Defaults to \code{1}}
%
\end{description}

\subsubsection{Sampling Adaptation-Specific Parameters}

\begin{figure}
\setlength{\unitlength}{0.005in}
\centering
\begin{picture}(1000, 200)
%
\footnotesize
\put(25, 20) { \framebox(75, 200)[c]{I} }
\put(100, 20) { \framebox(25, 200)[c]{II} }
\put(125, 20) { \framebox(50, 200)[c]{II} }
\put(175, 20) { \framebox(100, 200)[c]{II} }
\put(275, 20) { \framebox(200, 200)[c]{II} }
\put(475, 20) { \framebox(400, 200)[c]{II} }
\put(875, 20) { \framebox(50, 200)[c]{III} }
\put(25, 20) { \vector(1, 0){950} }
\put(800, -10) { \makebox(200, 20)[l]{{\small Iteration}} }
%
\end{picture}
\caption{\small\it Adaptation during warmup occurs in three stages: an initial
fast adaptation interval (I), a series of expanding slow adaptation intervals (II),
and a final fast adaptation interval (III).  For HMC, both the fast and slow intervals
are used for adapting the step size, while the slow intervals are used for learning
the (co)variance necessitated by the metric.  Iteration numbering
starts at 1 on the left side of the figure and increases to the right.}%
\label{adaptation.figure}
\end{figure}

When adaptation is engaged the warmup period is split into three
stages (Figure \ref{adaptation.figure}), with two \textit{fast} intervals
surrounding a series of growing \textit{slow} intervals.  Here fast
and slow refer to parameters that adapt using local and global
information, respectively; the Hamiltonian Monte Carlo samplers,
for example, define the step size as a fast parameter and
the (co)variance as a slow parameter.  The size of the the initial
and final fast intervals and the initial size of the slow interval
are all customizable, although user-specified values may be modified
slightly in order to ensure alignment with the warmup period.

The motivation behind this partitioning of the warmup period is to
allow for more robust adaptation.  In the initial fast interval the
chain is allowed to converge towards the typical set,%
%
\footnote{The typical set is a concept borrowed from information
theory and refers to the neighborhood (or neighborhoods in multimodal models)
of significant posterior probability mass through which the Markov chain
will travel in equilibrium.}
%
with only parameters that can learn from local information adapted.
After this initial stage parameters that require global information,
for example (co)variances, are estimated in a series of expanding,
memoryless windows; often fast parameters will be adapted here
as well.  Lastly the fast parameters are allowed to adapt to the final
update of the slow parameters.

Currently all Stan sampling algorithms utilize dual averaging to
optimize the step size (this optimization during adaptation of the
sampler should not be confused with running Stan's optimization method).
This optimization procedure is extremely flexible and for completeness
we have exposed each option, using the notation of
\citep{Hoffman-Gelman:2011, Hoffman-Gelman:2014}.  In practice the
efficacy of the optimization is sensitive to the value of these
parameters, and we do not recommend changing the defaults without
experience with the dual averaging algorithm.  For more information,
see the discussion of dual averaging in \citep{Hoffman-Gelman:2011,
 Hoffman-Gelman:2014}.

Variances or covariances are estimated using Welford accumulators
to avoid a loss of precision over many floating point operations.

The following subarguments are introduced by the categorical argument
\code{adapt}.  Each subargument must contiguously follow \code{adapt},
though they may appear in any order.

\begin{description}
    \hierlongcmd{\indentarrow\indentarrow}{\bfseries adapt}
      {Warmup Adaptation}
      {Valid subarguments: \code{engaged, gamma, delta, kappa, t0}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{engaged}{$$boolean$$}
        {Adaptation engaged?}
        {Valid values: \ \code{0, 1}}
        {Defaults to \code{1}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{gamma}{$$double$$}
        {Adaptation regularization scale}
        {Valid values: \  $0 < \mbox{\code{gamma}}$}
        {Defaults to \code{0.05}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{delta}{$$double$$}
        {Adaptation target acceptance statistic}
        {Valid values: \  $0 < \mbox{\code{delta}} < 1$}
        {Defaults to \code{0.8}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{kappa}{$$double$$}
        {Adaptation relaxation exponent}
        {Valid values: \  $0 < \mbox{\tt kappa}$}
        {Defaults to \code{0.75}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{t0}{$$double$$}
        {Adaptation iteration offset}
        {Valid values: \  $0 < \mbox{\code{t0}}$}
        {Defaults to \code{10}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{init\_buffer}{$$unsigned int$$}
        {Width of initial fast adaptation interval}
        {Valid values: \ All}
        {Defaults to \code{75}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{term\_buffer}{$$unsigned int$$}
        {Width of final fast adaptation interval}
        {Valid values: \ All}
        {Defaults to \code{50}}
%
      \hiercmdarg{\indentarrow\indentarrow\indentarrow}{window}{$$unsigned int$$}
        {Initial width of slow adaptation interval}
        {Valid values: \ All}
        {Defaults to \code{25}}
%
\end{description}
%
By setting the acceptance statistic \code{delta} to a value closer to
1 (its value must be strictly less than 1 and its default value is
0.8), adaptation will be forced to use smaller step sizes.  This can
improve sampling efficiency (effective samples per iteration) at the
cost of increased iteration times.  Raising the value of \code{delta}
will also allow some models that would otherwise get stuck overcome
their blockages; see also the \code{stepsize\_jitter} argument.

\subsubsection{Sampling Algorithm- and Engine-Specific Arguments}

The following batch of arguments are used to control the sampler used
for sampling.  The top-level specification is for engine, the only
valid value of which is \code{hmc} (this will change in the future as
we add new samplers).

\begin{description}
    \hiercmdarg{\indentarrow\indentarrow}{algorithm}{$$list element$$}
      {Sampling algorithm}
      {Valid values: \  \code{hmc}, \code{fixed\_param} }
      {Defaults to \code{hmc}}
\end{description}
%
Hamiltonian Monte Carlo is a very general approach to sampling that
utilizes techniques of differential geometry and mathematical physics
to generate efficient MCMC transitions.  This generality manifests in
a wealth of implementation choices.
%
\begin{description}
      \hierlongcmd{\indentarrow\indentarrow\indentarrow}{\farg{\bfseries hmc}}
        {Hamiltonian Monte Carlo}
        {Valid subarguments: \code{engine, metric, stepsize, stepsize\_jitter}}
\end{description}
%
All HMC implementations require at least two parameters: an
integration step size and a total integration time.  We refer to
different specifications of the latter as \textit{engines}.

In the \code{static\_hmc} implementation the total integration time
must be specified by the user, where as the \code{nuts}
implementation uses the No-U-Turn Sampler to determine
an optimal integration time dynamically.
%
\begin{description}
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{engine}{$$list element$$}
          {Engine for Hamiltonian Monte Carlo}
          {Valid values: \  \code{static, nuts}}
          {Defaults to \code{nuts}}
\end{description}
%
The following options are activated for static HMC.
%
\begin{description}
          \hierlongcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{\bfseries static}}
            {Static integration time}
            {Valid subarguments: \code{int\_time}}
%
            \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{int\_time}{$$double$$}
              {Total integration time for Hamiltonian evolution}
              {Valid values: \  $0 < \mbox{\code{int\_time}}$}
              {Defaults to $2\pi$}
\end{description}
%
These options are for NUTS, an adaptive version of HMC.
%
\begin{description}
          \hierlongcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{\bfseries nuts}}
            {The No-U-Turn Sampler}
            {Valid subarguments: \code{max\_depth}}
%
\end{description}

\subsubsection{Tree Depth}

NUTS generates a proposal by evolving the initial system both forwards
and backwards in time to form a balanced binary tree.  At each iteration
of the NUTS algorithm the tree depth is increased by one, doubling
the number of leapfrog steps and effectively doubles the computation
time.  The algorithm terminates in one of two ways: either the NUTS
criterion is satisfied for a new subtree or the completed tree, or the
depth of the completed tree hits \code{max\_depth}.

Both the tree depth and the actual number of leapfrog steps computed are
reported along with the parameters in the output as \code{treedepth\_\_} and
\code{n\_leapfrog\_\_}, respectively.  Because the final subtree may only
be partially constructed, these two will always satisfy
%
\begin{equation*}
2^{\mathrm{treedepth} - 1} - 1 < N_{\mathrm{leapfrog}} \le 2^{\mathrm{treedepth} } - 1.
\end{equation*}

\code{treedepth\_\_} is an important diagnostic tool for NUTS.  For example,
\code{treedepth\_\_ = 0} occurs when the first leapfrog step is immediately
rejected and the initial state returned, indicating extreme curvature and
poorly-chosen step size (at least relative to the current position).  On the other
hand, if \code{treedepth\_\_ = max\_depth} then NUTS is taking many leapfrog
steps and being terminated prematurely to avoid excessively long execution
time.  For the most efficient sampling \code{max\_depth} should be increased
to ensure that the NUTS tree can grow as large as necessary.

For more information on the NUTS algorithm see \citep{Hoffman-Gelman:2011, Hoffman-Gelman:2014}.

\begin{description}
            \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{max\_depth}{$$int$$}
              {Maximum tree depth}
              {Valid values: \  $0 < \mbox{\code{max\_depth}}$}
              {Defaults to \code{10}}
%
\end{description}
%



\subsubsection{Euclidean Metric}

All HMC implementations in Stan utilize quadratic kinetic energy
functions which are specified up to the choice of a symmetric,
positive-definite matrix known as a \textit{mass matrix} or, more
formally, a \textit{metric} \citep{Betancourt-Stein:2011}.

If the metric is constant then the resulting implementation is known
as \textit{Euclidean} HMC.  Stan allows for three Euclidean HMC
implementations: a unit metric, a diagonal metric, and a dense
metric.  These can be specified with the values \code{unit\_e},
\code{diag\_e}, and \code{dense\_e}, respectively.

Future versions of Stan will also include dynamic metrics associated
with \textit{Riemannian} HMC \citep{GirolamiCalderhead:2011, Betancourt:2012}.

By default, the metric is estimated during warmup.
However, when using a \code{diag\_e} or \code{dense\_e} metric, an initial guess
for the metric can be specified with the \code{metric\_file} argument.
If provided, the \code{metric\_file} should contain a single variable,
\code{inv\_metric}, which for a \code{diag\_e} metric should be a vector
of positive values, one for each parameter in the system. For a \code{dense\_e}
metric, \code{inv\_metric} should be a positive-definite square matrix with
number of rows and columns equal to the number of parameters in the model.
The file pointed at by \code{metric\_file} should use either JSON
formate or the data dump format (the
same format the input data uses). For examples of specifying a vector in the
data dump format (as is needed for specifying a \code{diag\_e} metric),
see \refsection{sequence-variables}. For examples of specifying a matrix in the
data dump format (as is needed for specifying a \code{dense\_e} metric),
see \refsection{array-variables}.

The \code{metric\_file} option can be used with and without adaptation enabled.

If adaptation is enabled, the provided metric will be used as the initial
guess in the adaptation process. If the initial guess is good, then adaptation
should not change it much. If the metric is no good, then the adaptation will
override the initial guess. If adaptation is enabled, \code{num\_warmup} must be
set to a value greater than zero. An example of running the sampler with
adaptation enabled without specifying a stepsize is

\begin{quote}
  \begin{Verbatim}[fontshape=sl]
    > ./model method=sample algorithm=hmc  \
              metric_file=model.metric.data.R  \
              data file=model.data.R init=model.init.R
  \end{Verbatim}
\end{quote}

If adaptation is disabled, both a \code{metric\_file} and \code{stepsize}
should be provided to the sampler. Disabling adaptation disables both metric
and stepsize adaptation, so a stepsize should be provided along with a metric
to enable efficient sampling. An example of providing a \code{metric\_file}
and \code{stepsize} with adaptation disabled is

\begin{quote}
  \begin{Verbatim}[fontshape=sl]
    > ./model method=sample adapt engaged=0 algorithm=hmc  \
              metric_file=model.metric.data.R  \
              stepsize=0.1 data file=model.data.R  \
              init=model.init.R
  \end{Verbatim}
\end{quote}

%
\begin{description}
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{metric}{$$list element$$}
          {Geometry of base manifold}
          {Valid values: \  \code{unit\_e, diag\_e, dense\_e}}
          {Defaults to \code{diag\_e}}
%
          \hiershortcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{unit\_e}}
            {Euclidean manifold with unit metric}
%
          \hiershortcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{diag\_e}}
            {Euclidean manifold with diag metric}
%
          \hiershortcmd{\indentarrow\indentarrow\indentarrow\indentarrow\indentarrow}{\farg{dense\_e}}
            {Euclidean manifold with dense metric}

        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{metric\_file}{$$string$$}
          {Input file with precomputed Euclidean metric}
          {Valid values: \  Valid path}
          {Defaults to empty path}
\end{description}
%

\subsubsection{Step Size and Jitter}

All implementations of HMC also use numerical integrators requiring a
step size.  We also allow that step size to be ``jittered'' randomly
during sampling to avoid any poor interactions with a fixed step size
and regions of high curvature.  The maximum amount of jitter is 1,
which will cause step sizes to be selected in the range of 0 to twice
the adapted step size.  Low step sizes can get HMC samplers unstuck
that would otherwise get stuck with higher step sizes.  The downside
is that jittering below the adapted value will increase the number of
leapfrog steps required and thus slow down iterations, whereas
jittering above the adapted value can cause premature rejection due to
simulation error in the Hamiltonian dynamics calculation.  See
\citep{Neal:2011} for further discussion of step-size jittering.
%
\begin{description}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{stepsize}{$$double$$}
          {Step size for discrete evolution}
          {Valid values: \  $0 < \code{stepsize}$}
          {Defaults to \code{1}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{stepsize\_jitter}{$$double$$}
          {Uniformly random jitter of the stepsize, in percent}
          {Valid values: \  $0 \leq \mbox{\code{stepsize\_jitter}} \leq 1$}
          {Defaults to \code{0}}
\end{description}

\subsubsection{Fixed Parameter Sampler}

The fixed parameter sampler generates a new sample without changing
the current state of the Markov chain; only generated quantities may
change.  This can be useful when, for example, trying to generate pseudo-data
using the generated quantities block. If the parameters block is empty (no parameters) then using {\tt algorithm=fixed\_param} is mandatory.
%
\begin{description}
      \hierlongcmd{\indentarrow\indentarrow\indentarrow}{\farg{\bfseries fixed\_param}}
        {Fixed Parameter Sampler}
\end{description}

\subsection{Optimization-Specific Commands}

The following arguments are for the top-level method \code{optimize}.
They allow control of the optimization algorithm, and some of its
configuration.  The other arguments may appear in any order.

\begin{description}
%
  \hierlongcmd{\indentarrow}{\farg{\bfseries optimize}}
    {Point estimation}
    {Valid subarguments: \code{algorithm, iter, save\_iterations}}
%
    \hiercmdarg{\indentarrow\indentarrow}{algorithm}{$$list element$$}
      {Optimization algorithm}
      {Valid values: \  \code{bfgs, lbfgs, newton}}
      {Defaults to \code{lbfgs}}
\end{description}
%
The following options are for the (L-)BFGS optimizer. L-BFGS is the default
optimizer and also much faster than the other optimizers.

Convergence monitoring in (L-)BFGS is controlled by a number of tolerance
values, any one of which being satisfied causes the algorithm to
terminate with a solution.
%
\begin{itemize}
\item The log probability is considered to have converged if
\[
\left| \log p(\theta_{i}|y) - \log p(\theta_{i-1}|y) \right| <
\mbox{\code{tol\_obj}}
\]
or
\[
\frac{\left| \log p(\theta_{i}|y) - \log p(\theta_{i-1}|y) \right|}{\
\max\left(\left| \log p(\theta_{i}|y)\right|,\left| \log p(\theta_{i-1}|y)\right|,1.0\right)}
 < \mbox{\code{tol\_rel\_obj}} * \epsilon.
\]
\item The parameters are considered to have converged if
%
\\
\[
|| \theta_{i} - \theta_{i-1} || < \mbox{\code{tol\_param}}.
\]
%
\item The gradient is considered to have converged to 0 if
\[
|| g_{i} || < \mbox{\code{tol\_grad}}
\]
or
\[
\frac{g_{i}^T \hat{H}_{i}^{-1} g_{i} }{ \max\left(\left|\log p(\theta_{i}|y)\right|,1.0\right) } < \mbox{\code{tol\_rel\_grad}} * \epsilon.
\]
\end{itemize}
%
Here, $i$ is the current iteration, $\theta_{i}$ is the value of the
parameters at iteration $i$, $y$ is the data, $p(\theta_{i}|y)$ is
the posterior probability of $\theta_{i}$ up to a proportion,
$\nabla_{\theta}$ is the gradient operator with respect to $\theta$,
$g_{i} = \nabla_{\theta} \log p(\theta_{i}|y)$ is the gradient at
iteration $i$, $\hat{H}_{i}$ is the estimate of the Hessian at
iteration $i$, $|u|$ is absolute value (L1 norm) of $u$,
$||u||$ is vector length (L2 norm) of $u$, and $\epsilon \approx 2e-16$ is
machine precision.  Any of the convergence tests can be disabled
by setting its corresponding tolerance parameter to zero.

The other command-line argument for (L-)BFGS is \code{init\_alpha},
which is the first step size to try on the initial iteration. If the
first iteration takes a long time (and requires a lot of function
evaluations), set \code{init\_alpha} to be the roughly equal to the
alpha used in that first iteration.  \code{init\_alpha} has a tiny
default value, which is reasonable for many problems but might be too
large or too small depending on the objective function and
initialization. Being too big or too small just means that the first
iteration will take longer (i.e., require more gradient evaluations)
before the line search finds a good step length. It's not a critical
parameter, but for optimizing the same model multiple times (as
you tweak things or with different data) being able to change it can
save some real time.

Finally, L-BFGS has a additional command-line argument, \code{history\_size},
which controls how much memory is used maintaining the approximation of
the Hessian.  This should be less than the dimensionality of the parameter
space and, in general, relatively small values (5 - 10) are sufficient.
If L-BFGS performs badly but BFGS is performing well, then consider increasing
this.  Note that increasing this will increase the memory usage, although
this is unlikely to be an issue for typical Stan models.
%
\begin{description}
      \hierlongcmd{\indentarrow\indentarrow\indentarrow}{\farg{\bfseries (l)bfgs}}
        {(L-)BFGS with linesearch}
        {Valid subarguments: \code{init\_alpha}, \code{tol\_obj}, \code{tol\_rel\_obj}, \code{tol\_grad}, \code{tol\_rel\_grad}, \code{tol\_param}, \code{history\_size} (lbfgs only)}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{init\_alpha}{$$double$$}
           {Line search step size for first iteration}
           {Valid values: $0 \leq \mbox{\code{init\_alpha}}$}
           {Defaults to \code{0.001}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_obj}{$$double$$}
           {Convergence tolerance on changes in objective function value}
           {Valid values: $0 \leq \mbox{\code{tol\_obj}}$}
           {Defaults to \code{1e-12}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_rel\_obj}{$$double$$}
           {Convergence tolerance on relative changes in objective function value}
           {Valid values: $0 \leq \mbox{\code{tol\_rel\_obj}}$}
           {Defaults to \code{1e+4}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_grad}{$$double$$}
        {Convergence tolerance on the norm of the gradient}
        {Valid values: $0 \leq \mbox{\code{tol\_grad}}$}
        {Defaults to \code{1e-8}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_rel\_grad}{$$double$$}
        {Convergence tolerance on the relative norm of the gradient}
        {Valid values: $0 \leq \mbox{\code{tol\_rel\_grad}}$}
        {Defaults to \code{1e+7}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{tol\_param}{$$double$$}
        {Convergence tolerance on changes in parameter value}
        {Valid values: $0 \leq \mbox{\code{tol\_param}}$}
        {Defaults to \code{1e-8}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{history\_size}{$$int$$}
           {Number of update vectors to use in Hessian approximations (lbfgs only)}
           {Valid values: $0 < \mbox{\code{history\_size}}$}
           {Defaults to \code{5}}
%
\end{description}
%
The following argument is for Newton's optimization method;  there are
currently no configuration parameters for Newton's method, and it is
not recommended because of the slow Hessian calculation involving
finite differences.
%
\begin{description}
      \hiershortcmd{\indentarrow\indentarrow\indentarrow}{\farg{\bfseries
          newton}}
        {Newton's method}
%
\end{description}
%
The remaining arguments apply to all optimizers.
\begin{description}
%
    \hiercmdarg{\indentarrow\indentarrow}{iter}{$$int$$}
      {Total number of iterations}
      {Valid values: \  $0 < \mbox{\code{iter}}$}
      {Defaults to \code{2000}}
%
    \hiercmdarg{\indentarrow\indentarrow}{save\_iterations}{$$boolean$$}
      {Stream optimization progress to output?}
      {Valid values: \  \ \code{0, 1}}
      {Defaults to \code{0}}
%
\end{description}

\subsection{Variational Inference-Specific Commands}

The following arguments are for the top-level method \code
{variational}.
They allow control of the variational inference algorithm, and some of its
configuration.

\begin{description}
%
  \hierlongcmd{\indentarrow}{\farg{\bfseries variational}}
    {Variational inference}
    {Valid subarguments: \code{algorithm, iter, grad\_samples, elbo\_samples,
    eta, adapt, tol\_rel\_obj, eval\_elbo, output\_samples}}
%
    \hiercmdarg{\indentarrow\indentarrow}{algorithm}{$$list element$$}
      {Variational inference algorithm}
      {Valid values: \ \code{meanfield, fullrank}}
      {Defaults to \code{meanfield}}
%
    \hiercmdarg{\indentarrow\indentarrow}{iter}{$$int$$}
      {Maximum number of iterations}
      {Valid values: \ $0 < \mbox{\code{iter}}$}
      {Defaults to \code{10000}}
%
    \hiercmdarg{\indentarrow\indentarrow}{grad\_samples}{$$int$$}
      {Number of samples for Monte Carlo estimate of gradients}
      {Valid values: \ $0 < \mbox{\code{grad\_samples}}$}
      {Defaults to \code{1}}
%
    \hiercmdarg{\indentarrow\indentarrow}{elbo\_samples}{$$int$$}
      {Number of samples for Monte Carlo estimate of ELBO (objective function)}
      {Valid values: \ $0 < \mbox{\code{elbo\_samples}}$}
      {Defaults to \code{100}}
%
    \hiercmdarg{\indentarrow\indentarrow}{eta}{$$double$$}
      {Stepsize weighting parameter for adaptive stepsize sequence}
      {Valid values: \ $0 < \mbox{\code{eta}}$}
      {Defaults to \code{1.0}}
%
    \hierlongcmd{\indentarrow\indentarrow}{\textbf{adapt}}
      {Warmup Adaptation}
      {Valid subarguments: \code{engaged, iter}}
%
    \hiercmdarg{\indentarrow\indentarrow\indentarrow}{engaged}{$$boolean$$}
        {Adaptation engaged?}
        {Valid values: \ \code{0, 1}}
        {Defaults to \code{1}}
%
    \hiercmdarg{\indentarrow\indentarrow\indentarrow}{iter}{$$int$$}
        {Maximum number of adaptation iterations}
        {Valid values: \  $0 < \mbox{\code{iter}}$}
        {Defaults to \code{50}}
%

    \hiercmdarg{\indentarrow\indentarrow}{tol\_rel\_obj}{$$double$$}
      {Convergence tolerance on the relative norm of the objective}
      {Valid values: \ $0 < \mbox{\code{tol\_rel\_obj}}$}
      {Defaults to \code{0.01}}
%
    \hiercmdarg{\indentarrow\indentarrow}{eval\_elbo}{$$int$$}
      {Evaluate ELBO every Nth iteration}
      {Valid values: \ $0 < \mbox{\code{eval\_elbo}}$}
      {Defaults to \code{100}}
%
    \hiercmdarg{\indentarrow\indentarrow}{output\_samples}{$$int$$}
      {Number of posterior samples to draw and save}
      {Valid values: \ $0 < \mbox{\code{output\_samples}}$}
      {Defaults to \code{1000}}
\end{description}
%
% The following options are for the (L-)BFGS optimizer.  BFGS is the default
% optimizer and also much faster than the other optimizers.

% Convergence monitoring in (L-)BFGS is controlled by a number of tolerance
% values, any one of which being satisfied causes the algorithm to
% terminate with a solution.

\subsection{Diagnostic-Specific Arguments}

The following arguments are specific to diagnostics.  As of now, the
only diagnostic is gradients of the log probability function.

\begin{description}
%
  \hierlongcmd{\indentarrow}{\farg{\bfseries diagnose}}
    {Model diagnostics}
    {Valid subarguments: \code{test}}
%
    \hiercmdarg{\indentarrow\indentarrow}{test}{$$list element$$}
      {Diagnostic test}
      {Valid values: \  \code{gradient}}
      {Defaults to \code{gradient}}
%
      \hiershortcmd{\indentarrow\indentarrow\indentarrow}{\farg{gradient}}
        {Check model gradient against finite differences}
        {Valid subarguments: \code{epsilon}, \code{error}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{epsilon}{$$real$$}
        {Finite difference step size}
	    {Valid values:\ $0 < \mbox{\code{epsilon}}$}
	    {Defaults to \code{1e-6}}
%
        \hiercmdarg{\indentarrow\indentarrow\indentarrow\indentarrow}{error}{$$real$$}
       {Error threshold}
	   {Valid values:\ $0 < \mbox{\code{error}}$}
	   {Defaults to \code{1e-6}}
%
\end{description}

\subsection{Generate\_quantities-Specific Arguments}

The following arguments are for the top-level method \code
{generate\_quantities}.
They specify the parameter values used to generate additional quantities of interest
from the model conditioned on the data.

\begin{description}
%
  \hierlongcmd{\indentarrow}{\farg{\bfseries generate\_quantities}}
    {Generate quantities of interest using sample of fitted parameter values}
    {Valid subarguments: \code{fitted\_params}}
%
    \hiercmdarg{\indentarrow\indentarrow}{fitted\_params}{$$string$$}
      {Input file of sample of fitted parameter values for model conditioned on data}
      {Valid values: \ Path to existing file}
      {Defaults to empty path}
%
\end{description}



\subsection{General-Purpose Arguments}

The following arguments may be used with any of the previous
configurations.   They may come either before or after the other
subarguments of the top-level method.

\subsubsection{Process Identifier Argument}

\begin{description}
\hiercmdarg{}{id}{$$int$$}
  {Unique process identifier, used to advance random number generator so that random numbers do not overlap across chains}
  {Valid values: \  $0 < \mbox{\code{id}}$}
  {Defaults to \code{0}}
%
\end{description}

\subsubsection{Input Data Arguments}

\begin{description}

\hierlongcmd{}{data}
  {Input data options}
  {Valid subarguments: \code{file}}
%
  \hiercmdarg{\indentarrow}{file}{$$string$$}
    {Input data file}
    {Valid values: \  Path to existing file}
    {Defaults to empty path}
%
\end{description}

\subsubsection{Initialization Arguments}

Initialization is only applied to parameters defined in the parameters
block.  Any initial values supplied for transformed parameters or
generated quantities are ignored.

\begin{description}
\hiercmdarg{}{init}{$$string$$}
  {Initialization method: \\
        \hspace*{8pt} $\bullet$ \ real number $\mbox{\farg{x}} > 0$ initializes randomly between [-\farg{x},
        \farg{x}];
        \\
        \hspace*{8pt} $\bullet$ \  \code{0} initializes to 0;
        \\
        \hspace*{8pt} $\bullet$ \  non-number interpreted as a data file}
  {Valid values: \  All}
  {Defaults to \code{2}}
%
\end{description}


\subsubsection{Random Number Generator Arguments}

\begin{description}

\hierlongcmd{}{{\bfseries random}}
  {Random number configuration}
  {Valid subarguments: \code{seed}}
%
  \hiercmdarg{\indentarrow}{seed}{$$unsigned int$$}
    {Random number generator seed}
    {Valid values: \\
      \hspace*{8pt} $\bullet$ \ $\mbox{\code{seed}} \geq 0$ generates seed;
      \\
      \hspace*{8pt} $\bullet$ \ $\mbox{\code{seed}} < 0$ uses seed generated from time}
    {Defaults to \code{-1}}
%
\end{description}

\subsubsection{Output Arguments}

\begin{description}
\hierlongcmd{}{{\bfseries output}}
  {File output options}
  {Valid subarguments: \code{file, diagnostic\_file, refresh}}
%
  \hiercmdarg{\indentarrow}{file}{$$string$$}
    {Output file}
    {Valid values: \  Valid path}
    {Defaults to \code{output.csv}}
%
  \hiercmdarg{\indentarrow}{diagnostic\_file}{$$string$$}
    {Auxiliary output file for diagnostic information}
    {Valid values: \  Valid path}
    {Defaults to empty path}
%
  \hiercmdarg{\indentarrow}{refresh}{$$int$$}
    {Number of iterations between screen updates}
    {Valid values: \  $0 < \mbox{\code{refresh}}$}
    {Defaults to \code{100}}
%
\end{description}


All Stan arguments have default values, except for the method.  This
is the only argument that must be specified by the user and a model
will not run without it (not to say that the model will run without error,
for example a model that requires data will eventually fail unless an input file
is specified with \code{file} under \code{data}).  Assuming that we want to draw
MCMC samples from our model, we can either specify a method
implicitly,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model sample data file=model.data.R init=model.init.R
\end{Verbatim}
\end{quote}
%
or explicitly,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample data file=model.data.R  \
          init=model.init.R
\end{Verbatim}
\end{quote}
%
In either case our model now executes without any problem.

Now let's say that we want to customize our execution.  In
particular we want to set the seed for the random number generator,
but we forgot the specific argument syntax.  Information for each
argument can displayed by calling \code{help},
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model random help
\end{Verbatim}
\end{quote}
%
which returns
%
\begin{quote}
\begin{Verbatim}
random
  Random number configuration
  Valid subarguments: seed
...
\end{Verbatim}
\end{quote}
%
before printing usage information.  For information on the
seed argument we just call help one level deeper,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model random seed help
\end{Verbatim}
\end{quote}
%
which returns
%
\begin{quote}
\begin{Verbatim}
seed=<unsigned int>
  Random number generator seed
  Valid values: seed > 0, if negative seed is generated from time
  Defaults to -1
  ...
\end{Verbatim}
\end{quote}
%
Fully informed, we can now run with a given seed,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample data fle=model.data.R  \
          init=model.init.R                    \
          random seed=5
\end{Verbatim}
\end{quote}

The arguments \code{method}, \code{data}, \code{init}, and
\code{random} are all top-level arguments.  To really see the power of
a hierarchical argument structure let's try to drill down and specify
the metric we use for HMC: instead of the default diagonal Euclidean
metric, we want to use a dense Euclidean metric.  Attempting to
specify the metric we try
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample data file=model.data.R  \
          init=model.init.R                     \
          random seed=5                         \
          metric=unit
\end{Verbatim}
\end{quote}
%
only to have the execution fail with the message
%
\begin{quote}
\begin{Verbatim}
metric=unit_e is either mistyped or misplaced.
Perhaps you meant one of the following valid configurations?
  method=sample algorithm=hmc metric=<list_element>
Failed to parse arguments, terminating Stan
\end{Verbatim}
\end{quote}
%
The argument \code{metric} does exist, but not at the top-level.  In order
to specify it we have to drill down into sample by first specifying the
sampling algorithm, as noted in the suggestion,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample algorithm=hmc metric=unit \
          data file=model.data.R                  \
          init=model.init.R                       \
          random seed=5
\end{Verbatim}
\end{quote}
%
Unfortunately we still messed up,
%
\begin{quote}
\begin{Verbatim}
unit is not a valid value for "metric"
  Valid values: unit_e, diag_e, dense_e
Failed to parse arguments, terminating Stan
\end{Verbatim}
\end{quote}
%
Tweaking the metric name we make one last attempt,
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample algorithm=hmc metric=unit_e \
          data file=model.data.R                    \
          init=model.init.R                         \
          random seed=5
\end{Verbatim}
\end{quote}
%
which successfully runs.

Finally, let's consider the circumstance where our model runs fine but
the NUTS iterations keep saturating the default tree depth limit of 10.  We need
to change the limit, but how do we specify NUTS let alone the maximum tree depth?
To see how let's take advantage of the \code{help-all} option which prints all
arguments that derive from the given argument.  We know that NUTS is somehow
related to sampling, so we try
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample help-all
\end{Verbatim}
\end{quote}
%
which returns the verbose output,
%
%
\begin{quote}
\begin{Verbatim}[fontsize=\small]
sample
  Bayesian inference with Markov Chain Monte Carlo
  Valid subarguments: num_samples, num_warmup,
                      save_warmup, thin, adapt, algorithm

  num_samples=<int>
    Number of sampling iterations
    Valid values: 0 <= num_samples
    Defaults to 1000

  num_warmup=<int>
    Number of warmup iterations
    Valid values: 0 <= warmup
    Defaults to 1000

  save_warmup=<boolean>
    Stream warmup samples to output?
    Valid values: [0, 1]
    Defaults to 0

  thin=<int>
    Period between saved samples
    Valid values: 0 < thin
    Defaults to 1

  adapt
    Warmup Adaptation
    Valid subarguments: engaged, gamma, delta, kappa, t0

    engaged=<boolean>
      Adaptation engaged?
      Valid values: [0, 1]
      Defaults to 1

    gamma=<double>
      Adaptation regularization scale
      Valid values: 0 < gamma
      Defaults to 0.05

    delta=<double>
      Adaptation target acceptance statistic
      Valid values: 0 < delta < 1
      Defaults to 0.65

    kappa=<double>
      Adaptation relaxation exponent
      Valid values: 0 < kappa
      Defaults to 0.75

    t0=<double>
      Adaptation iteration offset
      Valid values: 0 < t0
      Defaults to 10

  algorithm=<list element>
    Sampling algorithm
    Valid values: hmc
    Defaults to hmc

    hmc
      Hamiltonian Monte Carlo
      Valid subarguments: engine, metric, stepsize,
                          stepsize_jitter

      engine=<list element>
        Engine for Hamiltonian Monte Carlo
        Valid values: static, nuts
        Defaults to nuts

        static
          Static integration time
          Valid subarguments: int_time

          int_time=<double>
            Total integration time for Hamiltonian evolution
            Valid values: 0 < int_time
            Defaults to 2 * pi

        nuts
          The No-U-Turn Sampler
          Valid subarguments: max_depth

          max_depth=<int>
            Maximum tree depth
            Valid values: 0 < max_depth
            Defaults to 10

      metric=<list element>
        Geometry of base manifold
        Valid values: unit_e, diag_e, dense_e
        Defaults to diag_e

        unit_e
          Euclidean manifold with unit metric

        diag_e
          Euclidean manifold with diag metric

        dense_e
          Euclidean manifold with dense metric

      metric_file=<string>
        Input file with precomputed Euclidean metric
        Valid values: Path to existing file
        Defaults to ""

      stepsize=<double>
        Step size for discrete evolution
        Valid values: 0 < stepsize
        Defaults to 1

      stepsize_jitter=<double>
        Uniformly random jitter of the stepsize, in percent
        Valid values: 0 <= stepsize_jitter <= 1
        Defaults to 0
...
\end{Verbatim}
\end{quote}
%
Following the hierarchy, the maximum tree depth derives from \code{nuts},
which itself is a value for the argument \code{engine} which derives from
\code{hmc}.  Adding this to our previous call we attempt
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample                       \
              algorithm=hmc                   \
                  metric=unit_e               \
                  engine=nuts max_depth=-15   \
          data file=model.data.R              \
          init=model.init.R                   \
          random seed=5                       \
\end{Verbatim}
\end{quote}
%
which yields
%
\begin{quote}
\begin{Verbatim}
-1 is not a valid value for "max_depth"
  Valid values: 0 < max_depth
Failed to parse arguments, terminating Stan
\end{Verbatim}
\end{quote}
%
Where did that negative sign come from?  Clumsy fingers are nothing
to be embarrassed about, especially with such complex argument
configurations.  Removing the guilty character, we try
%
\begin{quote}
\begin{Verbatim}[fontshape=sl]
> ./model method=sample                     \
              algorithm=hmc                 \
                  metric=unit_e             \
                  engine=nuts max_depth=15  \
          data file=model.data.R            \
          init=model.init.R                 \
          random seed=5
\end{Verbatim}
\end{quote}
%
which finally runs without issue.

\section{Command Templates}

This section provides templates for all of the arguments deriving from
each of the possible methods: \code{sample}, \code{optimize}, \code{variational}
and \code {diagnose}.
Arguments in square brackets are optional,
those not in square brackets are required for the template.

\subsection{Sampling Templates}
%
The No-U-Turn sampler (NUTS) is the default (and recommended) sampler
for Stan.  The full set of configuration options is in
\reffigure{nuts-command}.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model sample                                     \
                 algorithm=hmc                          \
                     engine=nuts                        \
                       [max_depth=<int>]                \
                     [metric={unit_e,diag_e,dense_e}]   \
                     [metric_file=<string>]             \
                     [stepsize=<double>]                \
                     [stepsize_jitter=<double>]         \
                 [num_samples=<int>]                    \
                 [num_warmup=<int>]                     \
                 [save_warmup=<boolean>]                \
                 [thin=<int>]                           \
                 [adapt                                 \
                      [engaged=<boolean>]               \
                      [gamma=<double>]                  \
                      [delta=<double>]                  \
                      [kappa=<double>]                  \
                      [t0=<double>] ]                   \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the no-U-turn sampler
  (NUTS). This is the same skeleton as that for basic HMC in
  \reffigure{hmc-command}.  Elements in braces are optional.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{nuts-command.figure}
\end{figure}
%

A standard Hamiltonian Monte Carlo (HMC) sampler with user-specified
integration time may also be used.  Its set of configuration options
are shown in \reffigure{hmc-command}.

Both NUTS and HMC may be configured with either a unit,
diagonal or dense Euclidean metric, with a diagonal metric the default.%
%
\footnote{In Euclidean HMC, a diagonal metric emulates different step
sizes for each parameter.  Explicitly varying step sizes were used in
Stan 1.3 and before; \cite{Neal:2011} discusses the equivalence.}
%
A unit metric provides no parameter-by-parameter scaling,
a diagonal metric scales each parameter independently, and
a dense metric also rotates the parameters so
that correlated parameters may move together.  Although dense metrics
offer the hope of superior simulation performance, they
require more computation per iteration.  Specifically for $m$ samples of a model with
$n$ parameters, the dense metric requires $\mathcal{O}(n^3 \log(m) +
n^2 \, m)$ operations, whereas diagonal metrics require only
$\mathcal{O}(n \, m)$.  Furthermore, dense metrics are difficult
to estimate, given the $\mathcal{O}(n^2)$ components with complex
interdependence.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model sample                                     \
                 algorithm=hmc                          \
                     engine=static                      \
                       [int_time=<double>]              \
                     [metric={unit_e,diag_e,dense_e}]   \
                     [metric_file=<string>]             \
                     [stepsize=<double>]                \
                     [stepsize_jitter=<double>]         \
                 [num_samples=<int>]                    \
                 [num_warmup=<int>]                     \
                 [save_warmup=<boolean>]                \
                 [thin=<int>]                           \
                 [adapt                                 \
                      [engaged=<boolean>]               \
                      [gamma=<double>]                  \
                      [delta=<double>]                  \
                      [kappa=<double>]                  \
                      [t0=<double>] ]                   \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the basic Hamiltonian
  Monte Carlo sampler (HMC).  This is the same as the NUTS command
  skeleton shown in \reffigure{nuts-command} other than for the
  engine.  Elements in braces are optional.  All arguments and their
  default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{hmc-command.figure}
\end{figure}

\subsection{Optimization Templates}
%
CmdStan supports several optimizers.  These share many of their
configuration options with the samplers.  The default optimizer is the
the limited memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) method;
\cite{NocedalWright:2006} contains an excellent overview of both the
BFGS and L-BFGS algorithms.  The command skeleton for L-BFGS is in
\reffigure{l-bfgs-command} and the one for BFGS is in
\reffigure{bfgs-command}.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model optimize                                   \
                 algorithm=lbfgs                         \
                     [init_alpha=<double>]              \
                     [tol_obj=<double>]                 \
                     [tol_rel_obj=<double>]             \
                     [tol_grad=<double>]                \
                     [tol_rel_grad=<double>]            \
                     [tol_param=<double>]               \
                     [history_size=<int>]               \
                 [iter=<int>]                           \
                 [save_iterations=<boolean>]            \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the L-BFGS optimizer.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{l-bfgs-command.figure}
\end{figure}
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model optimize                                   \
                 algorithm=bfgs                         \
                     [init_alpha=<double>]              \
                     [tol_obj=<double>]                 \
                     [tol_rel_obj=<double>]             \
                     [tol_grad=<double>]                \
                     [tol_rel_grad=<double>]            \

                     [tol_param=<double>]               \
                 [iter=<int>]                           \
                 [save_iterations=<boolean>]            \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the BFGS optimizer.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{bfgs-command.figure}
\end{figure}
%
Stan also supports Newton's method; see \citep{NocedalWright:2006} for
more information.  This method is the least efficient of the three,
but has the advantage of setting its own step size.  Other than not
having a stepsize argument, the skeleton for Newton's method shown in
\reffigure{newton-command} is identical to that for BFGS.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model optimize                                   \
                 algorithm=newton                       \
                 [iter=<int>]                           \
                 [save_iterations=<boolean>]            \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the Newton optimizer.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{newton-command.figure}
\end{figure}
%

\subsection{Variational Inference Templates}
%
CmdStan implements Automatic Differentiation Variational Inference
\citep{Kucukelbir:2015}. The command skeleton for the \code{meanfield}
algorithm is in \reffigure{meanfield-command}. The command skeleton for the
\code{fullrank} algorithm is in \reffigure{fullrank-command}.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model variational                                \
                 algorithm=meanfield                    \
                     [iter=<int>]                       \
                     [grad_samples=<int>]               \
                     [elbo_samples=<int>]               \
                     [eta=<double>]                     \
                     [adapt                             \
                      [engaged=<boolean>]               \
                      [iter=<int>] ]                    \
                     [tol_rel_obj=<double>]             \
                     [eval_elbo=<int>]                  \
                     [output_samples=<int>]             \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the meanfield variational
  inference algorithm.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{meanfield-command.figure}
\end{figure}
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model variational                                \
                 algorithm=fullrank                     \
                     [iter=<int>]                       \
                     [grad_samples=<int>]               \
                     [elbo_samples=<int>]               \
                     [eta=<double>]                     \
                     [adapt                             \
                      [engaged=<boolean>]               \
                      [iter=<int>] ]                    \
                     [tol_rel_obj=<double>]             \
                     [eval_elbo=<int>]                  \
                     [output_samples=<int>]             \
             [data file=<string>]                       \
             [init=<string>]                            \
             [random seed=<int>]                        \
             [output                                    \
                  [file=<string>]                       \
                  [diagnostic_file=<string>]            \
                  [refresh=<int>] ]
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking the fullrank variational
  inference algorithm.
  All arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{fullrank-command.figure}
\end{figure}


\subsection{Diagnostic Command Skeleton}

Stan reports on gradients for the model at a specified or randomly
generated initial value.  The command-skeleton in this case is very
simple, and shown in \reffigure{diagnostic-command}.
%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model diagnose                  \
                 [test=gradient]       \
                     [epsilon=<real>]  \
                     [error=<real>]    \
             [data file=<string>]      \
             [init=<string>]           \
             [random seed=<int>]       \
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for invoking model diagnostics.  All
  arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{diagnostic-command.figure}
\end{figure}


\subsection{Generate\_quantities Command Skeleton}

The \code{generate\_quantities} method can be used to generate
additional quantities of interest from the model conditioned on the data.
The data used to fit the model should be specified as part of the command.

%
\begin{figure}
\begin{quote}
\begin{Verbatim}[fontshape=sl,fontsize=\small]
> ./my_model generate\_quantities                  \
                 [fitted\_params=<string>]       \
             [data file=<string>]      \
             [random seed=<int>]       \
\end{Verbatim}
\end{quote}
\caption{\small\it Command skeleton for generating new quantities of interest
from a set of fitted parameter values using \code{method=generate\_quantities}.  All
  arguments and their default values are described in detail in
  \refsection{detailed-command-arguments}.}
\label{generate-quantities-command.figure}
\end{figure}

